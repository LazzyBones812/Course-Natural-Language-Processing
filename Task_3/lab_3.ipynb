{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω—É–∂–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –±–ª–∏–∑–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\mArt\\dataScience\\NLP_SDP\\NLP\\homework\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\mArt\\dataScience\\NLP_SDP\\NLP\\homework\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from transformers import pipeline\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "''' –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è - –Ω–∞–ø—Ä–∏–º–µ—Ä, Java better than Python –∏ \n",
    "Python better than Java –±—É–¥—É—Ç –∏–º–µ—Ç—å –Ω–µ–±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –±–ª–∏–∑–æ—Å—Ç–∏ '''\n",
    "def sim_sent_1(orig, gen):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    tokens1 = tokenizer.tokenize(orig)\n",
    "    tokens2 = tokenizer.tokenize(gen)\n",
    "\n",
    "    input_ids1 = torch.tensor(tokenizer.convert_tokens_to_ids(tokens1)).unsqueeze(0)\n",
    "    input_ids2 = torch.tensor(tokenizer.convert_tokens_to_ids(tokens2)).unsqueeze(0)\n",
    "\n",
    "    outputs1 = model(input_ids1)\n",
    "    outputs2 = model(input_ids2)\n",
    "    embeddings1 = outputs1.last_hidden_state.detach().numpy()[:, 0, :]\n",
    "    embeddings2 = outputs2.last_hidden_state.detach().numpy()[:, 0, :]\n",
    "\n",
    "    similarity_score = cosine_similarity(embeddings1, embeddings2)\n",
    "    return similarity_score\n",
    "\n",
    "''' –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–ª–∏–∑–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é sentence-transformers'''\n",
    "def sim_sent_2(orig, gen):\n",
    "    model = SentenceTransformer('sentence-transformers/msmarco-bert-base-dot-v5')\n",
    "    embedding_1 = model.encode([orig])\n",
    "    embedding_2 = model.encode([gen])\n",
    "    similarity_score = cosine_similarity(embedding_1, embedding_2)\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø—É—Ç–µ–º –≤—Å—Ç–∞–≤–∫–∏ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ [MASK] –≤ —Å–ª—É—á–∞–π–Ω—ã–µ –º–µ—Å—Ç–∞ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "symb_mask = '[MASK]'\n",
    "text_orig = 'After your workout, remember to focus on maintaining a good water balance.'\n",
    "num_perf = random.randint(10, 25) # –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–º–µ–Ω —Å–ª–æ–≤ –Ω–∞ [MASK] \n",
    "\n",
    "res_text = []\n",
    "num_sent = 10 # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "\n",
    "for j in range(num_sent):\n",
    "    text = text_orig\n",
    "    for i in range(num_perf):\n",
    "        list_text = text.split(' ')\n",
    "        num_pos = random.randint(0, len(list_text) - 1)\n",
    "        list_text[num_pos] = '[MASK]'\n",
    "        text = ' '.join(list_text)\n",
    "        text = pipe(text)[0]['sequence']\n",
    "    res_text.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ –±–ª–∏–∑–æ—Å—Ç—å —Å –∏—Å—Ö–æ–¥–Ω—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence After your workout, remember to focus on maintaining a good water balance.\n",
      "Sentence # 1 :  continue your search and focus on finding a good water.\n",
      "Similarity Score by BERT: [[0.8046778]]\n",
      "Similarity Score by Sentence_Transformers: [[0.90852064]]\n",
      "Sentence # 2 :  in this case remember to focus on maintaining a good water balance.\n",
      "Similarity Score by BERT: [[0.73124075]]\n",
      "Similarity Score by Sentence_Transformers: [[0.9529843]]\n",
      "Sentence # 3 :  after your shower, to focus on getting a little.\n",
      "Similarity Score by BERT: [[0.6619318]]\n",
      "Similarity Score by Sentence_Transformers: [[0.9220751]]\n",
      "Sentence # 4 :  after your workout, you focus on finding a new body.\n",
      "Similarity Score by BERT: [[0.8130115]]\n",
      "Similarity Score by Sentence_Transformers: [[0.95211756]]\n",
      "Sentence # 5 :  during your workout, try to focus on maintaining a healthy body.\n",
      "Similarity Score by BERT: [[0.7770817]]\n",
      "Similarity Score by Sentence_Transformers: [[0.9583457]]\n",
      "Sentence # 6 :  let your training remember. focus on getting yourself.\n",
      "Similarity Score by BERT: [[0.6815591]]\n",
      "Similarity Score by Sentence_Transformers: [[0.91152155]]\n",
      "Sentence # 7 :  after your shower remember to focus on getting a little water.\n",
      "Similarity Score by BERT: [[0.8464827]]\n",
      "Similarity Score by Sentence_Transformers: [[0.9462176]]\n",
      "Sentence # 8 :  in this case remember to focus on maintaining a good mental balance.\n",
      "Similarity Score by BERT: [[0.74940157]]\n",
      "Similarity Score by Sentence_Transformers: [[0.9280671]]\n",
      "Sentence # 9 :  after your workout, try to focus on getting a good body balance.\n",
      "Similarity Score by BERT: [[0.9282815]]\n",
      "Similarity Score by Sentence_Transformers: [[0.9788413]]\n",
      "Sentence # 10 :  after your classes try to focus on building a new.\n",
      "Similarity Score by BERT: [[0.6846248]]\n",
      "Similarity Score by Sentence_Transformers: [[0.8928494]]\n"
     ]
    }
   ],
   "source": [
    "print('Original sentence', text_orig)\n",
    "for i, c in enumerate(res_text):\n",
    "    print('Sentence #', i + 1, ': ', c)\n",
    "    print(\"Similarity Score by BERT:\", sim_sent_1(text_orig, c))\n",
    "    print(\"Similarity Score by Sentence_Transformers:\", sim_sent_2(text_orig, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ—Ö–æ–¥—è—Ç –ø–æ—Ä–æ–≥ 0.75 –ø–æ –æ–±–æ–∏–º —Ñ–∞–∫—Ç–æ—Ä–∞–º** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = []\n",
    "for i, c in enumerate(res_text):\n",
    "    if (sim_sent_1(text_orig, c)[0][0] >= 0.75) & (sim_sent_2(text_orig, c)[0][0] >= 0.75):\n",
    "        result_1.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–í—Ç–æ—Ä–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ–±—ã –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å–ª–æ–≤–∞ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏, –Ω–æ –æ—Ç–±–∏—Ä–∞—Ç—å —Å–ª—É—á–∞–π–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –∏–∑ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø—è—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence After your workout, remember to focus on maintaining a good water balance\n",
      "Sentence # 1 :  during the swim needs must focused on obtaining generally optimal lateral balance\n",
      "Similarity Score by BERT: [[0.46790874]]\n",
      "Similarity Score by Sentence_Transformers: [[0.9047325]]\n",
      "Sentence # 2 :  in this, remember and concentrate upon getting this whole new ;\n",
      "Similarity Score by BERT: [[0.6498668]]\n",
      "Similarity Score by Sentence_Transformers: [[0.8819918]]\n",
      "Sentence # 3 :  throughout each season has the emphasis in finding another new musical |\n",
      "Similarity Score by BERT: [[0.58260113]]\n",
      "Similarity Score by Sentence_Transformers: [[0.84218776]]\n",
      "Sentence # 4 :  in your mind try to keep in it a certain mental...\n",
      "Similarity Score by BERT: [[0.54820824]]\n",
      "Similarity Score by Sentence_Transformers: [[0.8780413]]\n",
      "Sentence # 5 :  throughout each day tryent focus on drinking a bottled liquid!\n",
      "Similarity Score by BERT: [[0.607496]]\n",
      "Similarity Score by Sentence_Transformers: [[0.8875871]]\n"
     ]
    }
   ],
   "source": [
    "res_text = []\n",
    "num_sent = 5 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "text_orig = 'After your workout, remember to focus on maintaining a good water balance'\n",
    "\n",
    "for j in range(num_sent):\n",
    "    text = text_orig\n",
    "    for i in range(len(text.split(' '))):\n",
    "        try:\n",
    "            list_text = text.split(' ')\n",
    "            list_text[i] = '[MASK]'\n",
    "            text = ' '.join(list_text)\n",
    "            text = pipe(text)[random.randint(0, 4)]['sequence']\n",
    "        except:\n",
    "            break\n",
    "    res_text.append(text)\n",
    "\n",
    "\n",
    "\n",
    "print('Original sentence', text_orig)\n",
    "for i, c in enumerate(res_text):\n",
    "    print('Sentence #', i + 1, ': ', c)\n",
    "    print(\"Similarity Score by BERT:\", sim_sent_1(text_orig, c))\n",
    "    print(\"Similarity Score by Sentence_Transformers:\", sim_sent_2(text_orig, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ—Ö–æ–¥—è—Ç –ø–æ—Ä–æ–≥ 0.70 –∏ 0.75 –ø–æ BertTokenizer –∏ SentenceTransformer —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = []\n",
    "for i, c in enumerate(res_text):\n",
    "    if (sim_sent_1(text_orig, c)[0][0] >= 0.7) & (sim_sent_2(text_orig, c)[0][0] >= 0.75):\n",
    "        result_2.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–≤—É—Ö —Å–ø–∏—Å–∫–æ–≤ —Å–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['continue your search and focus on finding a good water.',\n",
       " 'after your workout, you focus on finding a new body.',\n",
       " 'during your workout, try to focus on maintaining a healthy body.',\n",
       " 'after your shower remember to focus on getting a little water.',\n",
       " 'after your workout, try to focus on getting a good body balance.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = result_1 + result_2\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
